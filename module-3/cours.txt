1/ Feature extraction / cleaning / solution ingeniering
2/ Objectis ?
Classification Accuracy nb de bonnes prediction/nb de prediction
3/ Protocol experimental
train set / test set /dev set
4/ Cross validation


ML workflow
1/Data cleaning feature enginering feature selection
2/ metrics / objectifs
- accuracy, F1, precision, recall
- RMSE, MSE, MAE
3/ protocol experimental
- train/dev/testt
- cross validation
4/ data cleaning
- reperer les valeurs maquantes
    - supprimer les observations
    - supprimer la feature ou colonne
    - inférer une valeur pour chaque val manquante (dangereux)
- reperer les outliers et les traiter
    - Si peu d'outliers/peu de données on peut les supprimer
    - si beaucoup d'outliers/de data on peut les garder
    
    
5/ Feature ingenering
Injecter de la connaissance metier en créeant des nouvelles colnnes/variable/feature à partir de variable existance
encerclage des variables
- categorical
- numeric
- other

6/ Feature selection
- Fléau de la dimension
- Overlifting

PCA
------------------------------------------------------------
Standardisation
retrancher la moyenne / ecart type
utilisé dans 90% des cas
StandardScaler

Normalisation
retranche par le min / ecart max - min

MaxAbsScaler
MinMaxScaler
-------------------------------------------------------------
Natural Language Processing (NLP)
Texte Image Audio
Comment representer le texte pour qu'il soit manipulable par le ML

Bag of words
Vesteur chaque mot est 0/1:
0 je
0 suis
0 il
1 nous
tous les mots sont independant les uns des autres
on perd le sens (terrible / pas terrible)

Separer les paragraphe en liste de phrases: sentence tokenizer
Ensuite tokeniser en mot: word tokenizer

Les tokenizer savent comment spliter en fonction de la langue "j'a" : pas d'espace
Count Vectorizer: transforme en vecteur le paragraphe
Il faut diminuer la taille du vocabulaire avant le countVecctorizer(car trop de feature)
- Supprimer les stop words car peu de valeur (prendre liste existante ou faire un count sur les mots pour les repérer)
- Supprimer les mots trop peu frequents (car pas assez nombreux pour apprendre au model)
- lowercser la totalité
- Stemming : couper chaque mot pour garder le radical attention dangereux flight = fli / finir = fin
- Lemmatisation: forme canonique du mot finira = finir / avait = avoir
- Regex
- TFIDF= Term frequency x Inverse doc Frequency = TF * log( #documents/#doc contenant le mot) : IfIdfVectorizer

Representation dense
base de donnée de vecteur deja existants
Algo deja existant qu'on peut appliqué sur notre corpus



































